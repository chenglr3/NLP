{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF（Term Frequency-Inverse Document Frequency）是一种在信息检索和文本挖掘中常用的权重计算方法，用于评估一个词语对于一个文档集或一个语料库中的其中一份文件的重要程度。它由两部分组成：词频（TF）和逆文档频率（IDF）。\n",
    "\n",
    "**词频（TF）** 是指某个词在文档中出现的频率，通常这个数字会进行归一化处理，以防止它偏向长的文件。词频的计算公式为：\n",
    "\\[ \\text{TF}(t, d) = \\frac{n_{t,d}}{N_d} \\]\n",
    "其中 \\( n_{t,d} \\) 是词 \\( t \\) 在文档 \\( d \\) 中出现的次数，\\( N_d \\) 是文档 \\( d \\) 中的总词数。\n",
    "\n",
    "**逆文档频率（IDF）** 是一种度量词项在文档中重要性的方式。如果包含词项的文档越少，则该词项的IDF值越大，表示该词项具有越强的类别区分能力。IDF的计算公式为：\n",
    "\\[ \\text{IDF}(t, D) = \\log\\frac{N}{df_t} \\]\n",
    "其中，\\( N \\) 是文档总数，\\( df_t \\) 是包含词 \\( t \\) 的文档数。在实际计算中，通常会在分母上加1，以避免某个词在语料库中没有出现时导致分母为零的情况。\n",
    "\n",
    "**TF-IDF** 计算公式为：\n",
    "\\[ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) \\]\n",
    "\n",
    "在构建特征矩阵时，每个文档会被表示为一个TF-IDF向量，其中每个维度对应一个词汇的TF-IDF值。这样，每个文档都由一个高维空间中的点表示，这个点的位置由TF-IDF值决定。\n",
    "\n",
    "**Python代码示例** 使用 `sklearn` 库中的 `TfidfVectorizer` 来构建TF-IDF特征矩阵：\n",
    "\n",
    "在这个例子中，`TfidfVectorizer` 会自动为我们完成计算TF-IDF值和构建特征矩阵的步骤。`X` 是一个稀疏矩阵，每一行对应一个文档，每一列对应一个词汇的TF-IDF值。\n",
    "\n",
    "**应用**：\n",
    "TF-IDF广泛应用于信息检索、文本分类、情感分析等领域。例如，在信息检索中，通过计算查询关键词的TF-IDF值，可以找到与查询最相关的文档；在文本分类中，TF-IDF向量可以作为分类器的输入特征。\n",
    "\n",
    "**优缺点**：\n",
    "- **优点**：简单易行，泛化能力强，可解释性好。\n",
    "- **缺点**：过分依赖词频，对文本语义理解不足，对停用词处理不当。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.78528828 0.6191303  0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.6191303  0.         0.         0.78528828\n",
      "  0.         0.        ]\n",
      " [0.48693426 0.         0.         0.         0.         0.\n",
      "  0.61761437 0.61761437]\n",
      " [0.48693426 0.         0.         0.61761437 0.61761437 0.\n",
      "  0.         0.        ]]\n",
      "['am' 'dream' 'have' 'learning' 'machine' 'pen' 'therefore' 'think']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"I have a dream\",\n",
    "    \"I have a pen\",\n",
    "    \"I think therefore I am\",\n",
    "    \"I am machine learning\",\n",
    "    # ... 更多文档\n",
    "]\n",
    "\n",
    "# 初始化TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 转换文本数据为TF-IDF特征矩阵\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 获取特征词汇\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 打印特征矩阵\n",
    "print(X.toarray())\n",
    "\n",
    "# 打印词汇\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
