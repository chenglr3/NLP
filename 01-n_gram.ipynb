{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram是一种统计语言模型，用于预测文本中下一个项目的概率。在自然语言处理中，n-gram可以用来构建文本的特征。n-gram模型基于假设，一个词的出现概率与它前面的n-1个词有关。n-gram模型可以捕捉到文本中的局部模式，对于文本分类、拼写检查、语音识别等任务非常有用。\n",
    "\n",
    "**原理：**\n",
    "假设有一个文本序列：\"I have a dream\"，如果我们使用bigram（2-gram）模型，就可以生成以下序列：\n",
    "- (I, have)\n",
    "- (have, a)\n",
    "- (a, dream)\n",
    "\n",
    "如果是trigram（3-gram）模型，则会生成：\n",
    "- (I, have, a)\n",
    "- (have, a, dream)\n",
    "\n",
    "n-gram模型会统计这些词组在语料库中出现的频率，然后用来预测下一个词或者进行其他NLP任务。\n",
    "\n",
    "**构建n-gram特征的步骤：**\n",
    "1. 选择n-gram的大小（n值）。\n",
    "2. 清洗和预处理文本（如分词、去除停用词等）。\n",
    "3. 根据n-gram大小，将文本分割成一系列的n-gram。\n",
    "4. 计算每个n-gram的出现频率。\n",
    "5. 使用n-gram特征进行机器学习建模。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['i', 'have', 'a', 'dream', 'that', 'one', 'day', 'this', 'nation', 'will', 'rise', 'up']\n",
      "defaultdict(<class 'int'>, {'i have': 1, 'have a': 1, 'a dream': 1, 'dream that': 1, 'that one': 1, 'one day': 1, 'day this': 1, 'this nation': 1, 'nation will': 1, 'will rise': 1, 'rise up': 1})\n",
      "tokens: ['i', 'have', 'a', 'dream', 'that', 'one', 'day', 'this', 'nation', 'will', 'rise', 'up']\n",
      "defaultdict(<class 'int'>, {'i have a': 1, 'have a dream': 1, 'a dream that': 1, 'dream that one': 1, 'that one day': 1, 'one day this': 1, 'day this nation': 1, 'this nation will': 1, 'nation will rise': 1, 'will rise up': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # 简单的分词，可以根据需要使用更复杂的分词方法,用于将文本分割成单词\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def create_n_grams(tokens, n):\n",
    "    # 用于生成n-gram\n",
    "    n_grams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(n_gram) for n_gram in n_grams]\n",
    "\n",
    "def n_gram_features(text, n=2):\n",
    "    # 用于计算每个n-gram的出现次数，并返回一个特征字典。\n",
    "    tokens = tokenize(text)\n",
    "    print('tokens:',tokens)\n",
    "    n_grams = create_n_grams(tokens, n)\n",
    "    features = defaultdict(int)\n",
    "    \n",
    "    for n_gram in n_grams:\n",
    "        features[n_gram] += 1\n",
    "        \n",
    "    return features\n",
    "\n",
    "# 示例文本\n",
    "text = \"I have a dream that one day this nation will rise up.\"\n",
    "\n",
    "# 生成bigram特征\n",
    "bigram_features = n_gram_features(text, n=2)\n",
    "print(bigram_features)\n",
    "\n",
    "# 生成trigram特征\n",
    "trigram_features = n_gram_features(text, n=3)\n",
    "print(trigram_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建特征矩阵是将文本数据转换为机器学习算法可以处理的数值型格式的过程。在自然语言处理中，一个常见的方法是使用词袋模型（Bag of Words, BoW），它将文本转换为单词出现次数的向量表示。对于bigram特征，这个过程是类似的，只是我们不仅考虑单个单词，还考虑单词对（即bigram）的出现次数。\n",
    "\n",
    "以下是构建bigram特征矩阵的步骤：\n",
    "\n",
    "1. **创建词汇表**：首先，需要从所有文档中提取所有出现过的bigram，并创建一个词汇表。词汇表是一个字典，它将每个bigram映射到一个唯一的整数索引。\n",
    "\n",
    "2. **计算bigram频率**：对于每个文档，计算词汇表中每个bigram的出现次数。这可以通过遍历文档中的每个单词对，并使用词汇表中的索引来实现。\n",
    "\n",
    "3. **构建特征向量**：对于每个文档，使用词汇表中的索引创建一个特征向量。向量的长度等于词汇表的大小，每个元素的值是对应bigram在文档中的出现次数。\n",
    "\n",
    "4. **构建特征矩阵**：对于所有文档，重复上述过程，最终得到一个特征矩阵，其中每一行代表一个文档，每一列代表一个bigram特征。\n",
    "\n",
    "5. **标准化/归一化**（可选）：为了消除文档长度对模型的影响，可以选择对特征向量进行标准化（例如，转换为TF-IDF值）或归一化（例如，每个特征向量的长度为1）。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，`CountVectorizer`自动为我们完成了创建词汇表、计算bigram频率和构建特征矩阵的步骤。`X`是一个稀疏矩阵，其中每一行对应一个文档，每一列对应一个bigram特征，矩阵中的值是bigram在文档中的出现次数。\n",
    "\n",
    "请注意，`CountVectorizer`返回的是一个稀疏矩阵，这是处理大型文本数据集时的一种内存高效的方式。在实际应用中，你可以根据需要将稀疏矩阵转换为密集矩阵，或者直接在稀疏矩阵上进行机器学习建模。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer: CountVectorizer(ngram_range=(2, 2))\n",
      "Feature matrix (terms x documents):\n",
      "[[0 1 2 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [1 0 0 0 1 0 0]]\n",
      "Bigram vocabulary (terms):\n",
      "['am machine' 'dream have' 'have dream' 'have pen' 'machine learning'\n",
      " 'therefore am' 'think therefore']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"I have a dream  have a dream\",\n",
    "    \"I have a pen\",\n",
    "    \"I think therefore I am\",\n",
    "    \"I am machine learning\",\n",
    "    # ... 更多文档\n",
    "]\n",
    "\n",
    "# 初始化CountVectorizer，设置ngram_range为2-gram\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "print('vectorizer:',vectorizer)\n",
    "\n",
    "# 转换文本数据为特征矩阵\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 获取特征词汇（bigram词汇表）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 打印特征矩阵（稀疏格式）\n",
    "print(\"Feature matrix (terms x documents):\")\n",
    "print(X.toarray())\n",
    "\n",
    "# 打印bigram词汇表\n",
    "print(\"Bigram vocabulary (terms):\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一个简单的Python代码示例，展示了如何使用bigram特征来训练一个逻辑回归分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"I have a dream\",\n",
    "    \"I have a pen\",\n",
    "    \"I think therefore I am\",\n",
    "    \"I am machine learning\",\n",
    "    # ... 更多文档\n",
    "]\n",
    "\n",
    "# 分类标签\n",
    "labels = [0, 1, 0, 1]  # 假设0和1是两类标签\n",
    "\n",
    "# 构建一个管道，包括词袋模型和逻辑回归分类器\n",
    "model = make_pipeline(CountVectorizer(analyzer='word', ngram_range=(2, 2)), LogisticRegression())\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
