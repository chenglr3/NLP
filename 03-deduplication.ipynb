{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **构造5个文档**，每个文档包含若干段落。\n",
    "2. **计算每个段落的SHA-1哈希值**，并保存为二进制文件。\n",
    "3. **去重**，删除出现次数超过3次的段落。\n",
    "\n",
    "\n",
    "### 代码说明：\n",
    "1. **构造文档**：使用一个字典`documents`，其中键是文档名称，值是文档中的段落列表。\n",
    "2. **计算哈希值**：定义`calculate_sha1`函数，使用SHA-1算法计算给定文本的哈希值。\n",
    "3. **保存哈希值**：定义`save_hash_to_binary`函数，将哈希值保存为二进制文件。\n",
    "4. **计算所有段落的哈希值并保存**：遍历所有文档和段落，计算哈希值并保存到字典`hashes`中，同时保存哈希值到二进制文件。\n",
    "5. **去重**：遍历所有文档和段落，如果段落的哈希值出现次数不超过3次，则保留该段落。\n",
    "6. **保存去重后的文档**：将去重后的段落写入原文档文件中。\n",
    "7. **打印去重后的文档内容**：遍历去重后的文档字典，打印每个文档的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "# 示例文档内容\n",
    "documents = {\n",
    "    \"doc1.txt\": [\"你好笨啊。\", \"这是文档1的第二个段落。\", \"这是文档1的第三个段落。\"],\n",
    "    \"doc2.txt\": [\"这是文档2的第一个段落。\", \"你好笨啊。\", \"这是文档2的第三个段落。\"],\n",
    "    \"doc3.txt\": [\"这是文档3的第一个段落。\", \"你好笨啊。\", \"这是文档3的第三个段落。\"],\n",
    "    \"doc4.txt\": [\"这是文档4的第一个段落。\", \"这是文档4的第二个段落。\", \"这是文档4的第三个段落。\"],\n",
    "    \"doc5.txt\": [\"这是文档5的第一个段落。\", \"这是文档5的第二个段落。\", \"你好笨啊。\"]\n",
    "}\n",
    "\n",
    "# 保存文档\n",
    "for doc_name, paragraphs in documents.items():\n",
    "    with open(doc_name, 'w', encoding='utf-8') as f:\n",
    "        for paragraph in paragraphs:\n",
    "            f.write(paragraph + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落的哈希字典，键是不同段落，值为sha-1值 {'c35769a2922cc98302015608decf880199768b45': 4, '234a2db10d761711a4d90ba4ffbf801206a20c1d': 1, '1695bf569bbcbbcbf317770d1fec6491b0ef4308': 1, 'eb4a8d7ec872dc674f6f2c7519ba7210c1f1160e': 1, '1104a2bc372b5cc6e112df15ebda2f98f59b2770': 1, 'e61b18a48553cf685fdd7a3032cbf2ece6c3e78d': 1, '1db57badeb80deb83f7b2d76e4c7d6a4d886e2a5': 1, '19c639068d368fb4fe69ebd810a23ead19ff9874': 1, '7e7379fd1891a1cae1a4aa5fae46f7120298abf7': 1, 'eefa0c218565e9f2f56669e2636fc41dd0b931b4': 1, '52e1ad7d7a09542753cf60d98e6f7034e76235f5': 1, '893ce23b00d383529a0e20a1ab9f6972fdb82fe2': 1}\n",
      "===去重后的文档内容===\n",
      "Document: doc1.txt\n",
      "这是文档1的第二个段落。\n",
      "这是文档1的第三个段落。\n",
      "\n",
      "\n",
      "Document: doc2.txt\n",
      "这是文档2的第一个段落。\n",
      "这是文档2的第三个段落。\n",
      "\n",
      "\n",
      "Document: doc3.txt\n",
      "这是文档3的第一个段落。\n",
      "这是文档3的第三个段落。\n",
      "\n",
      "\n",
      "Document: doc4.txt\n",
      "这是文档4的第一个段落。\n",
      "这是文档4的第二个段落。\n",
      "这是文档4的第三个段落。\n",
      "\n",
      "\n",
      "Document: doc5.txt\n",
      "这是文档5的第一个段落。\n",
      "这是文档5的第二个段落。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算SHA-1哈希值\n",
    "def calculate_sha1(text):\n",
    "    sha1 = hashlib.sha1()\n",
    "    sha1.update(text.encode('utf-8'))\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "# 保存哈希值到二进制文件\n",
    "def save_hash_to_binary(hash_value):\n",
    "    with open(f\"{hash_value}.bin\", \"wb\") as f:\n",
    "        f.write(hash_value.encode('utf-8'))\n",
    "\n",
    "# 计算所有段落的哈希值并保存\n",
    "hashes = {}\n",
    "for doc_name, paragraphs in documents.items():\n",
    "    for paragraph in paragraphs:\n",
    "        hash_value = calculate_sha1(paragraph)\n",
    "        if hash_value not in hashes:\n",
    "            hashes[hash_value] = 0\n",
    "        hashes[hash_value] += 1\n",
    "        save_hash_to_binary(hash_value)\n",
    "print('段落的哈希字典，键是不同段落，值为sha-1值',hashes)\n",
    "\n",
    "\n",
    "# 去重，删除出现次数超过3次的段落\n",
    "deduped_documents = {}\n",
    "for doc_name, paragraphs in documents.items():\n",
    "    deduped_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # 每个段落计算一个SHA-1值\n",
    "        hash_value = calculate_sha1(paragraph)\n",
    "        if hashes[hash_value] <= 3:\n",
    "            deduped_paragraphs.append(paragraph)\n",
    "    deduped_documents[doc_name] = deduped_paragraphs\n",
    "\n",
    "# 保存去重后的文档\n",
    "for doc_name, paragraphs in deduped_documents.items():\n",
    "    with open(doc_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for paragraph in paragraphs:\n",
    "            f.write(paragraph + \"\\n\")\n",
    "\n",
    "# 打印去重后的文档内容\n",
    "print('===去重后的文档内容===')\n",
    "for doc_name, paragraphs in deduped_documents.items():\n",
    "    print(f\"Document: {doc_name}\")\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
